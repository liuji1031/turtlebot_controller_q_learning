{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c68cf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pybullet as p\n",
    "import time\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "import numpy as np\n",
    "# define a class for the agent\n",
    "class WanderBot:\n",
    "    def __init__(self,max_ray_dist, max_ray_angle, n_ray_per_side, ray_sensor_height,\n",
    "                 default_speed, no_obs_tau, min_no_obs_t, update_dt, left_tau, right_tau,\n",
    "                cam_width=640, cam_height=480, center_thresh=0.05,reward_scale=1000):\n",
    "        self.max_ray_dist = max_ray_dist\n",
    "        self.max_ray_angle = max_ray_angle\n",
    "        self.n_ray_per_side = n_ray_per_side\n",
    "        if n_ray_per_side==1:\n",
    "            self.ray_angle_delta = max_ray_angle\n",
    "        else:\n",
    "            self.ray_angle_delta = max_ray_angle / (n_ray_per_side-0.5)\n",
    "        self.ray_sensor_height = ray_sensor_height\n",
    "        self.default_speed = default_speed\n",
    "        self.no_obs_cumu_t = 0.0 \n",
    "        self.no_obs_tau = no_obs_tau\n",
    "        self.min_no_obs_t = min_no_obs_t\n",
    "        self.no_obs_state = 0\n",
    "        self.update_dt = update_dt\n",
    "        self.left_tau = left_tau\n",
    "        self.left_cumu_t = 0.\n",
    "        self.right_tau = right_tau\n",
    "        self.right_cumu_t = 0.\n",
    "        self.cam_w = cam_width\n",
    "        self.cam_h = cam_height\n",
    "        self.center_thresh = center_thresh\n",
    "        self.reward_scale = reward_scale\n",
    "    \n",
    "    def get_current_position_and_orientation(self,p,agent):\n",
    "        position, quaternion = p.getBasePositionAndOrientation(agent)\n",
    "        x = position[0]\n",
    "        y = position[1]\n",
    "        eular_angle = p.getEulerFromQuaternion(quaternion)\n",
    "        yaw = eular_angle[2] # only need yaw\n",
    "        return x,y,yaw\n",
    "    \n",
    "    def dist_to_target(self,p,agent,cpos):\n",
    "        x,y,_ = self.get_current_position_and_orientation(p,agent)\n",
    "        d = np.power(x-cpos[0],2)+np.power(y-cpos[1],2)\n",
    "        d = np.sqrt(d)\n",
    "        return d\n",
    "    \n",
    "    def get_image(self,p,agent):\n",
    "        x,y,yaw = self.get_current_position_and_orientation(p,agent)\n",
    "        h = 0.2\n",
    "        d1 = 0.2\n",
    "        cam_pos = [x+d1*np.cos(yaw),y+d1*np.sin(yaw),h]\n",
    "        d = 1.\n",
    "        cam_target_pos = [x+d*np.cos(yaw),y+d*np.sin(yaw),h]\n",
    "        cam_up_vec = [0.,0.,1.0]\n",
    "        view_matrix = p.computeViewMatrix(cam_pos,cam_target_pos,cam_up_vec)\n",
    "        projection_matrix = p.computeProjectionMatrixFOV(\n",
    "                            fov=90, aspect=4./3., nearVal=0.1, farVal=10.0)\n",
    "        width = 640\n",
    "        height = 480\n",
    "        data = p.getCameraImage(width,\n",
    "                          height,\n",
    "                          view_matrix,\n",
    "                          projection_matrix,\n",
    "                          shadow=True,\n",
    "                          renderer=p.ER_BULLET_HARDWARE_OPENGL)\n",
    "        \n",
    "        return data[2]\n",
    "    \n",
    "    def get_object_size_center(self,image):\n",
    "        im_hsv = cv2.cvtColor(image,cv2.COLOR_RGB2HSV)\n",
    "        mask = cv2.inRange(im_hsv,(40,0,0),(60,255,255))\n",
    "        size = np.sum(mask==255)\n",
    "        if size>0:\n",
    "            M = cv2.moments(mask)\n",
    "            # calculate x,y coordinate of center\n",
    "            cX = M[\"m10\"] / M[\"m00\"]\n",
    "            cY = M[\"m01\"] / M[\"m00\"]\n",
    "            cX = cX/self.cam_w\n",
    "            cY = cY/self.cam_h\n",
    "            #print(cX,cY,size)\n",
    "            size_re = size / self.cam_w / self.cam_h\n",
    "            return (cX,cY),size_re\n",
    "        else:\n",
    "            return (-1,-1),0.0\n",
    "    \n",
    "    def compute_reward(self,center,size):\n",
    "        x = center[0]\n",
    "        x0 = (x-self.cam_w/2)/self.cam_w\n",
    "        if np.abs(x0) < self.center_thresh/2:\n",
    "            r = self.reward_scale*size/self.cam_w/self.cam_h\n",
    "        else:\n",
    "            r = 0\n",
    "        return r\n",
    "        \n",
    "    # send ray for testing obstacle\n",
    "    def ray_test(self,side,p,agent):\n",
    "        # first figure out the position and orientation of the robot\n",
    "        x,y,yaw = self.get_current_position_and_orientation(p,agent)\n",
    "        # print(x,y,yaw)\n",
    "        # construct the list of rays to send\n",
    "        rayFrom = []\n",
    "        rayTo = []\n",
    "        init_angle = self.ray_angle_delta/2\n",
    "        delta_angle = self.ray_angle_delta\n",
    "        if side==\"right\":\n",
    "            init_angle = -init_angle\n",
    "            delta_angle = -delta_angle\n",
    "        \n",
    "        for i in range(self.n_ray_per_side):\n",
    "            rayFrom.append([x,y,self.ray_sensor_height])\n",
    "            rayAngle = yaw + init_angle + i*delta_angle\n",
    "            rayTo.append([x+self.max_ray_dist*np.cos(rayAngle),\n",
    "                          y+self.max_ray_dist*np.sin(rayAngle),\n",
    "                          self.ray_sensor_height])\n",
    "        #print(rayTo)\n",
    "        # send array\n",
    "        results = p.rayTestBatch(rayFrom,rayTo)\n",
    "        hit_object_id = []\n",
    "        n_ray_hit = 0\n",
    "        for result in results:\n",
    "            hit_object_id.append(result[0])\n",
    "            if result[0]!=-1:\n",
    "                # count how many rays hit an object\n",
    "                # this number is used to drive motor\n",
    "                n_ray_hit += 1 \n",
    "    \n",
    "        return n_ray_hit,hit_object_id,rayTo\n",
    "    \n",
    "    def set_motor_velocity(self,p,agent,left_vel,right_vel):\n",
    "        p.setJointMotorControl2(agent,0,p.VELOCITY_CONTROL,\n",
    "                                targetVelocity=left_vel*self.default_speed,force=1000)\n",
    "        p.setJointMotorControl2(agent,1,p.VELOCITY_CONTROL,\n",
    "                                targetVelocity=right_vel*self.default_speed,force=1000)\n",
    "        \n",
    "    def check_no_obs_state(self):\n",
    "        # determine whether the robot enters the no-obstacle\n",
    "        # state, which prob increases linearly after the min\n",
    "        # time threshold\n",
    "        ct = self.no_obs_cumu_t - self.min_no_obs_t\n",
    "        ct = max(0.,ct)\n",
    "        prob = ct / self.no_obs_tau\n",
    "        prob = min(1.,prob)\n",
    "        if np.random.rand() < prob and self.no_obs_state == 0:\n",
    "            # print(\"entered no obs state\")\n",
    "            self.no_obs_state = 1\n",
    "            self.left_cumu_t = 0.0\n",
    "            self.right_cumu_t = 0.0\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    def get_motor_velocity(self,n_ray_left,n_ray_right,left_vel,right_vel):\n",
    "        if n_ray_right > 0 or n_ray_left > 0:\n",
    "            # there is obstacle, reset no-obstacle state\n",
    "            self.no_obs_cumu_t = 0.0\n",
    "            self.no_obs_state = 0\n",
    "        else:\n",
    "            # otherwise increase time counter\n",
    "            self.no_obs_cumu_t += self.update_dt\n",
    "            # check whether entered no-obstacle state\n",
    "            self.check_no_obs_state()\n",
    "        \n",
    "        if self.no_obs_state == 0 : # there is obstacle\n",
    "            dv_obs = 0.5\n",
    "            if n_ray_right > 0:\n",
    "                left_vel -= dv_obs*(np.random.rand())\n",
    "            else:\n",
    "                left_vel += dv_obs*(np.random.rand())\n",
    "            if n_ray_left > 0:\n",
    "                right_vel -= dv_obs*(np.random.rand())\n",
    "            else:\n",
    "                right_vel += dv_obs*(np.random.rand())\n",
    "        \n",
    "        else: # in no obstacle state, vary left and right wheel \n",
    "              # speed according to other rule\n",
    "                \n",
    "            # increase time counter\n",
    "            self.left_cumu_t += self.update_dt\n",
    "            self.right_cumu_t += self.update_dt\n",
    "            \n",
    "            # vary motor speed according to time in no-obstacle state\n",
    "            left_vel = self.vary_motor_vel(\"left\",left_vel)\n",
    "            right_vel = self.vary_motor_vel(\"right\",right_vel)\n",
    "            \n",
    "            # by default, slowly increase speed of both wheels\n",
    "            dv_no_obs = 0.05\n",
    "            left_vel += dv_no_obs*(np.random.rand())\n",
    "            right_vel += dv_no_obs*(np.random.rand())\n",
    "        \n",
    "        left_vel = min(left_vel, 1.0)\n",
    "        left_vel = max(left_vel,-1.0)\n",
    "        \n",
    "        right_vel = min(right_vel, 1.0)\n",
    "        right_vel = max(right_vel,-1.0)\n",
    "           \n",
    "        return left_vel, right_vel\n",
    "        \n",
    "    def vary_motor_vel(self,side,vel):\n",
    "        if side == \"left\":\n",
    "            prob = self.left_cumu_t / self.left_tau\n",
    "        else:\n",
    "            prob = self.right_cumu_t / self.right_tau\n",
    "        prob = min(1.0,prob)\n",
    "        tmp = np.random.rand()\n",
    "        if tmp < prob:\n",
    "            # reset time counter\n",
    "            if side == \"left\":\n",
    "                # print(\"left wheel speed varied\",tmp,prob,self.left_cumu_t)\n",
    "                self.left_cumu_t = 0.0\n",
    "            else:\n",
    "                # print(\"right wheel speed varied\",tmp,prob,self.right_cumu_t)\n",
    "                self.right_cumu_t = 0.0\n",
    "            \n",
    "            # randomly choose a vel between 0 and 1\n",
    "            return np.random.rand()\n",
    "        else:\n",
    "            # otherwise return the original velocity\n",
    "            return vel\n",
    "\n",
    "class RL_agent():\n",
    "    def __init__(self,n_wheel_state,hori_state_edge,size_state_edge,epsilon,\n",
    "                 alpha,gamma,reward_bin):\n",
    "        n_action_per_wheel = 3 # either increase/no change/decrease\n",
    "        self.wheel_state_list = np.linspace(-1.0,1.0,n_wheel_state,endpoint=True)\n",
    "        self.vel_step = self.wheel_state_list[1] - self.wheel_state_list[0]\n",
    "        self.hori_state_edge = hori_state_edge\n",
    "        self.size_state_edge = size_state_edge\n",
    "        n_hori_state = len(hori_state_edge)-1\n",
    "        n_size_state_edge = len(size_state_edge)-1\n",
    "        self.Q = np.zeros((n_hori_state,n_size_state_edge,n_wheel_state,n_wheel_state,\n",
    "                           n_action_per_wheel,n_action_per_wheel))\n",
    "        # the state: 1. horizontal location of the target object (0 to 1)\n",
    "        #            2. left wheel velocity (-1 to 1)\n",
    "        #            3. right wheel velocity (-1 to 1)\n",
    "        self.h = 0.\n",
    "        self.s = 0.\n",
    "        self.left_w_vel = 0.\n",
    "        self.right_w_vel = 0.\n",
    "        self.action_list = [-self.vel_step,0.,self.vel_step]\n",
    "        self.naction = len(self.action_list)\n",
    "        self.At = np.ones((3,2),dtype=np.int16) # default 0 velocity\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reward_bin = reward_bin\n",
    "        \n",
    "    def get_hori_state_ind(self,h):\n",
    "        for i in range(len(self.hori_state_edge)-1):\n",
    "            if h>=self.hori_state_edge[i] and h<self.hori_state_edge[i+1]:\n",
    "                return i\n",
    "        i = len(self.hori_state_edge)-2\n",
    "        if h>=self.hori_state_edge[i] and h<=self.hori_state_edge[i+1]:\n",
    "            return i\n",
    "    \n",
    "    def get_size_state_ind(self,s):\n",
    "        for i in range(len(self.size_state_edge)-1):\n",
    "            if s>=self.size_state_edge[i] and s<self.size_state_edge[i+1]:\n",
    "                return i\n",
    "        i = len(self.size_state_edge)-2\n",
    "        if s>=self.size_state_edge[i] and s<=self.size_state_edge[i+1]:\n",
    "            return i\n",
    "    \n",
    "    def get_wheel_state_ind(self,w_vel):\n",
    "        ind = int((w_vel - (-1.0))/self.vel_step)\n",
    "        return ind\n",
    "    \n",
    "    def get_state_ind(self):\n",
    "        iH = self.get_hori_state_ind(self.h)\n",
    "        iS = self.get_size_state_ind(self.s)\n",
    "        iWL = self.get_wheel_state_ind(self.left_w_vel)\n",
    "        iWR = self.get_wheel_state_ind(self.right_w_vel)\n",
    "        return [iH,iS,iWL,iWR]\n",
    "    \n",
    "    def select_action(self,h):\n",
    "        p = np.random.rand()\n",
    "        if p<self.epsilon: # select the action with largest Q\n",
    "            iH,iS,iWL,iWR = self.get_state_ind()\n",
    "            Q_this = self.Q[iH,iS,iWL,iWR,:,:]\n",
    "            # find max\n",
    "            Q_max = np.max(Q_this.flatten())\n",
    "            imax = np.argwhere(Q_this==Q_max)\n",
    "            i0 = np.random.permutation(imax.shape[0])[0]\n",
    "            iL = imax[i0,0]\n",
    "            iR = imax[i0,1]\n",
    "        else:\n",
    "            # random select\n",
    "            iL = np.random.permutation(self.naction)[0]\n",
    "            iR = np.random.permutation(self.naction)[0]\n",
    "        dVL = self.action_list[iL]\n",
    "        dVR = self.action_list[iR]\n",
    "        return [iL,iR],dVL,dVR\n",
    "    \n",
    "    def update_Q(self,last_state,last_action,curr_state,reward):\n",
    "        Q_last = self.Q[last_state[0],last_state[1],last_state[2],last_state[3],\n",
    "                        last_action[0],last_action[1]]\n",
    "        Q_max = np.max(self.Q[curr_state[0],curr_state[1],curr_state[2],curr_state[3],:,:])\n",
    "        self.Q[last_state[0],last_state[1],last_state[2],last_state[3],\n",
    "                        last_action[0],last_action[1]] = Q_last + self.alpha*(reward+self.gamma*Q_max-Q_last)\n",
    "    \n",
    "    def update_vel(self,dVL,dVR):\n",
    "        self.left_w_vel += dVL\n",
    "        self.right_w_vel += dVR\n",
    "        self.left_w_vel = min(self.left_w_vel,1.0)\n",
    "        self.left_w_vel = max(self.left_w_vel,-1.0)\n",
    "        self.right_w_vel = min(self.right_w_vel,1.0)\n",
    "        self.right_w_vel = max(self.right_w_vel,-1.0)\n",
    "    \n",
    "    def set_hs(self,h,s):\n",
    "        self.h = h\n",
    "        self.s = s\n",
    "        \n",
    "    def compute_reward(self,size):\n",
    "        iH = self.get_hori_state_ind(self.h)\n",
    "        if iH == self.reward_bin:\n",
    "            r = size\n",
    "        else:\n",
    "            r = -1\n",
    "        return r\n",
    "    \n",
    "    def save(self):\n",
    "        import pickle # for saving data\n",
    "        # create time stamp\n",
    "        from datetime import datetime\n",
    "        now = datetime.now() # current date and time\n",
    "        date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        # create file name\n",
    "        fn = \"RL_agent_\"+date_time+\".data\"\n",
    "        # save data\n",
    "        with open(fn, 'wb') as file2save:\n",
    "            pickle.dump(self, file2save)\n",
    "        print(\"Data saved to \",fn)\n",
    "    \n",
    "    def load(self,fn):\n",
    "        import pickle\n",
    "        with open(fn, 'rb') as file:\n",
    "            return pickle.load(file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da5c182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconnecting...\n",
      "-------- episode  1 ---------\n",
      "curr state [3, 3, 2, 2] curr action [2, 0] reward -1 distance 2.828457616708146 percent 50.47222222222222\n",
      "curr state [3, 3, 3, 4] curr action [1, 1] reward -1 distance 2.3890773486192756 percent 50.47222222222222\n",
      "curr state [3, 4, 3, 4] curr action [2, 0] reward -1 distance 2.073919440232415 percent 50.47222222222222\n",
      "curr state [3, 3, 3, 3] curr action [1, 2] reward -1 distance 1.7471300833511991 percent 50.47222222222222\n",
      "curr state [2, 4, 4, 4] curr action [1, 1] reward 5.771484375 distance 1.296014571880692 percent 50.47222222222222\n",
      "curr state [2, 5, 3, 3] curr action [0, 0] reward 16.259765625 distance 0.853512372032726 percent 50.47222222222222\n",
      "curr state [1, 5, 2, 2] curr action [0, 2] reward -1 distance 0.7338705021656022 percent 50.47222222222222\n",
      "done\n",
      "Data saved to  RL_agent_2023-06-09_20-18-38.data\n"
     ]
    }
   ],
   "source": [
    "use_gui = True\n",
    "\n",
    "try:\n",
    "    p.disconnect()\n",
    "except:\n",
    "    print(\"reconnecting...\")\n",
    "    \n",
    "if use_gui:\n",
    "    p.connect(p.GUI)\n",
    "    p.setRealTimeSimulation(1)\n",
    "    # the default delta t for pybullet is 1/240 sec\n",
    "    sim_dt = 1./240.\n",
    "    p.setTimeStep(sim_dt)\n",
    "    p.resetDebugVisualizerCamera( cameraDistance=4, cameraYaw=0,\n",
    "                    cameraPitch=-45, cameraTargetPosition=[1.0,0,0])\n",
    "else:\n",
    "    p.connect(p.DIRECT)\n",
    "    p.setRealTimeSimulation(0)\n",
    "    sim_dt = 1./100.\n",
    "    p.setTimeStep(sim_dt)\n",
    "    \n",
    "p.setGravity(0,0,-9.8)\n",
    "\n",
    "# spawn ground plane\n",
    "plane = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "\n",
    "# spawn the cylindral obstacles\n",
    "cylinder_pos_array = [[2.0,-2.0]]\n",
    "cylinder_list = []\n",
    "for i,cpos in enumerate(cylinder_pos_array):\n",
    "    cylinder = p.loadURDF(\"cylinder.urdf\",basePosition=[cpos[0],cpos[1],0.05],useFixedBase=1)\n",
    "    cylinder_list.append(cylinder)\n",
    "    \n",
    "# spawn turtle bot\n",
    "init_pos = [0.,0.,0.]\n",
    "init_orientation = [0.,0.,0.,1.]\n",
    "turtle = p.loadURDF(\"turtlebot.urdf\",init_pos,init_orientation)\n",
    "time.sleep(5.0)\n",
    "# specify the delta t for the agent update its, which is different\n",
    "# from simulation time step\n",
    "agent_update_dt = 0.020\n",
    "n_update = int(np.round(agent_update_dt / sim_dt))\n",
    "\n",
    "wb = WanderBot(max_ray_dist=0.4, max_ray_angle=60.0/180.0*np.pi, n_ray_per_side=7, \n",
    "               ray_sensor_height=0.45,default_speed=5.,no_obs_tau=1.0, min_no_obs_t=0.5, \n",
    "               update_dt = agent_update_dt, left_tau=2.0,right_tau=2.0,\n",
    "               cam_width=640, cam_height=480, center_thresh=0.08,reward_scale=1000)\n",
    "\n",
    "size_edge = np.logspace(-4,np.log10(0.5),8,endpoint=True)\n",
    "size_edge = np.hstack((0.0,size_edge))\n",
    "\n",
    "RL = RL_agent(n_wheel_state=5,\n",
    "                hori_state_edge=[-1.0,0.0,0.4,0.6,1.0],\n",
    "                size_state_edge=size_edge,\n",
    "                epsilon=0.5,alpha=0.5,gamma=0.9,reward_bin=2)\n",
    "\n",
    "use_prev = 1\n",
    "test_mode = 1\n",
    "if use_prev==1:\n",
    "    RL = RL.load('RL_agent_2023-06-09_19-29-42.data')\n",
    "\n",
    "    if test_mode:\n",
    "        RL.epsilon = 1.0\n",
    "        RL.alpha = 0.0\n",
    "    else:\n",
    "        RL.epsilon = 0.9\n",
    "        RL.alpha = 0.5\n",
    "    # reset wheel velocity\n",
    "    RL.left_w_vel = 0.\n",
    "    RL.right_w_vel = 0.\n",
    "\n",
    "\n",
    "# maximum duration for each episode in seconds\n",
    "max_dur_per_episode = 30.0\n",
    "if test_mode:\n",
    "    max_episode = 1\n",
    "else:\n",
    "    max_episode = 50\n",
    "i_episode = 0\n",
    "stop_all=False\n",
    "while stop_all == False:\n",
    "    # reset model\n",
    "    p.resetBasePositionAndOrientation(turtle,init_pos,init_orientation)\n",
    "    \n",
    "    # start the current episode\n",
    "    stop = False\n",
    "    t_lapsed = 0.\n",
    "    i = 0\n",
    "\n",
    "    # start video recording \n",
    "    from datetime import datetime\n",
    "    now = datetime.now() # current date and time\n",
    "    date_time = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    vidLogger = p.startStateLogging(p.STATE_LOGGING_VIDEO_MP4, \"Turtlebot_RL_Controller_\"+date_time+\".mp4\")\n",
    "\n",
    "    # intialize state\n",
    "    image = wb.get_image(p,turtle)\n",
    "    center,size = wb.get_object_size_center(image)\n",
    "    h = center[0]\n",
    "    RL.set_hs(h,size)\n",
    "    last_state = RL.get_state_ind()\n",
    "    last_action = [1,1] # 1 is dV = 0\n",
    "\n",
    "    turtle_pos = []\n",
    "\n",
    "    time.sleep(1.0)\n",
    "    if i_episode>=1 and i_episode%5==0:\n",
    "        RL.alpha = RL.alpha*0.9\n",
    "    print('-------- episode ',i_episode+1,'---------')\n",
    "    while stop==False:\n",
    "        if use_gui:\n",
    "            time.sleep(1./240.)\n",
    "            keys = p.getKeyboardEvents()\n",
    "            for k,v in keys.items():\n",
    "                    if (k == ord('q') and (v&p.KEY_WAS_TRIGGERED)):\n",
    "                        stop=True\n",
    "        else:\n",
    "            p.stepSimulation()\n",
    "\n",
    "        if i % n_update == 0:\n",
    "            x,y,yaw = wb.get_current_position_and_orientation(p,turtle)\n",
    "            \n",
    "            turtle_pos.append([x,y])\n",
    "            # get current state\n",
    "            image = wb.get_image(p,turtle)\n",
    "            center,size = wb.get_object_size_center(image)\n",
    "            h = center[0]\n",
    "            RL.set_hs(h,size)\n",
    "            curr_state = RL.get_state_ind()\n",
    "\n",
    "            # get reward for last action\n",
    "            reward = RL.compute_reward(size*1e3) \n",
    "\n",
    "            # select next action based on current state\n",
    "            curr_action,dVL,dVR = RL.select_action(center[0])\n",
    "\n",
    "            # use reward to update last state\n",
    "            RL.update_Q(last_state,last_action,curr_state,reward)\n",
    "\n",
    "            # cache last state and action\n",
    "            last_state = curr_state\n",
    "            last_action = curr_action\n",
    "\n",
    "            # carry out the action\n",
    "            RL.update_vel(dVL, dVR)\n",
    "        # command wheel speed\n",
    "        wb.set_motor_velocity(p,turtle,RL.left_w_vel,RL.right_w_vel)\n",
    "\n",
    "        d = wb.dist_to_target(p,turtle,cylinder_pos_array[0])\n",
    "        if d>=5 or d<=0.7:\n",
    "            stop = True\n",
    "\n",
    "        if i % int(200) == 0:\n",
    "            # print(center,size,reward)\n",
    "            p_update = sum(RL.Q.flatten()!=0)/RL.Q.size*100\n",
    "            print('curr state',curr_state,'curr action',curr_action,'reward',reward,'distance',d,'percent',p_update)\n",
    "        i+=1\n",
    "\n",
    "        t_lapsed += sim_dt\n",
    "        if t_lapsed>=max_dur_per_episode:\n",
    "            stop = True\n",
    "        if stop:\n",
    "            break\n",
    "    i_episode+=1\n",
    "    turtle_pos = np.array(turtle_pos)\n",
    "    if p_update>=50 or i_episode>=max_episode:\n",
    "        stop_all=True\n",
    "\n",
    "print('done')\n",
    "wb.set_motor_velocity(p,turtle,0,0)\n",
    "time.sleep(5.0)\n",
    "p.stopStateLogging(vidLogger)\n",
    "p.disconnect()\n",
    "\n",
    "RL.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
